# ELT-Bench
The first comprehensive, end-to-end benchmark designed to evaluate AI agents in automating ELT pipelines.
![ELT](https://anonymous.4open.science/r/ELT-Bench-B51C/materials/elt.svg)

## Table of Contents
- [Overview](#overview)
- [Project Structure](#project-structure)
  - [Directory Layout (Before Setup)](#directory-layout-before-setup)
  - [How Setup Transforms the Structure](#how-setup-transforms-the-structure)
  - [Directory Roles and Responsibilities](#directory-roles-and-responsibilities)
- [Workflow Overview](#workflow-overview)
- [Environment Setup](#environment-setup)
- [Running Agents](#running-agents)
- [Evaluation](#evaluation)
- [Common Issues and Solutions](#common-issues-and-solutions)
- [Project Maintenance](#project-maintenance)

## Overview

ELT-Bench is a benchmark suite containing **100 ELT pipeline problems** that test AI agents' ability to:
1. **Extract** data from various sources (PostgreSQL, MongoDB, S3, APIs, files)
2. **Load** data into Snowflake
3. **Transform** data according to specified requirements

The project uses a **two-tier structure**:
- **`elt-bench/`**: Read-only benchmark definitions (committed to Git)
- **`data/`**: Generated working environment (gitignored, recreated by setup)

## Project Structure

### Directory Layout (Before Setup)

```
ELT-Bench/
â”œâ”€â”€ README.md                           # Project documentation
â”‚
â”œâ”€â”€ elt-bench/                          # ğŸ“¦ BENCHMARK DEFINITIONS (100 problems)
â”‚   â”œâ”€â”€ address/                       # Problem 1: Address data
â”‚   â”‚   â”œâ”€â”€ config.yaml                # Data source configurations
â”‚   â”‚   â”œâ”€â”€ data_model.yaml            # Required output schema
â”‚   â”‚   â””â”€â”€ schemas/                   # Source data schemas
â”‚   â”œâ”€â”€ airline/                       # Problem 2: Airline data
â”‚   â””â”€â”€ ... (98 more problems)         # Problems 3-100
â”‚
â”œâ”€â”€ setup/                              # ğŸ”§ SETUP SCRIPTS & CREDENTIALS
â”‚   â”œâ”€â”€ elt_setup.sh                   # Main setup orchestrator
â”‚   â”œâ”€â”€ write_config.py                # Generates data/inputs/ from elt-bench/
â”‚   â”œâ”€â”€ data_setup.sh                  # Database initialization script
â”‚   â”œâ”€â”€ check_job_status.py            # Job monitoring helper (copied to inputs)
â”‚   â”œâ”€â”€ main.tf                        # Terraform template (copied to inputs)
â”‚   â”œâ”€â”€ mongo.py                       # MongoDB data loader
â”‚   â”œâ”€â”€ requirements.txt               # Python dependencies
â”‚   â”œâ”€â”€ elt_bench.yaml                 # Airbyte connector definition
â”‚   â”‚
â”‚   â”œâ”€â”€ airbyte/                       # Airbyte credentials (user-filled)
â”‚   â”‚   â””â”€â”€ airbyte_credential.json    # {username, password, workspace_id, api_definition_id}
â”‚   â”‚
â”‚   â”œâ”€â”€ destination/                   # Snowflake credentials (user-filled)
â”‚   â”‚   â”œâ”€â”€ snowflake_credential.json  # {account, user, password}
â”‚   â”‚   â””â”€â”€ setup.sql                  # Snowflake setup SQL
â”‚   â”‚
â”‚   â””â”€â”€ sources/                       # Source database setup scripts
â”‚       â”œâ”€â”€ postgres_setup.sql         # PostgreSQL schema & data
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ elt-docker/                         # ğŸ³ DOCKER INFRASTRUCTURE
â”‚   â”œâ”€â”€ docker-compose.yml             # Defines: PostgreSQL, MongoDB, S3 (LocalStack), APIs
â”‚   â””â”€â”€ rest_api/                      # (Created by setup) API source data
â”‚
â”œâ”€â”€ evaluation/                         # âœ… EVALUATION FRAMEWORK
â”‚   â”œâ”€â”€ eva.py                         # Main evaluation orchestrator
â”‚   â”œâ”€â”€ eva_stage1.py                  # Stage 1: Data extraction/loading validator
â”‚   â”œâ”€â”€ eva_stage2.py                  # Stage 2: Data transformation validator
â”‚   â”œâ”€â”€ table.json                     # Table metadata for validation
â”‚   â”‚
â”‚   â””â”€â”€ address/, airline/, ... (100)  # Expected SQL queries per problem
â”‚       â””â”€â”€ *.sql                      # Query definitions for comparison
â”‚
â”œâ”€â”€ agents/                             # ğŸ¤– AGENT IMPLEMENTATIONS
â”‚   â”œâ”€â”€ spider-agent/                  # Spider Agent implementation
â”‚   â””â”€â”€ SWE-agent/                     # SWE Agent implementation
â”‚
â”œâ”€â”€ documentation/                      # ğŸ“š API/PROVIDER DOCUMENTATION
â”‚   â”œâ”€â”€ connection.md                  # Connection guidelines
â”‚   â”œâ”€â”€ source_postgres.md             # PostgreSQL source docs
â”‚   â”œâ”€â”€ source_mongodb_v2.md           # MongoDB source docs
â”‚   â”œâ”€â”€ source_s3.md                   # S3 source docs
â”‚   â”œâ”€â”€ source_file.md                 # File source docs
â”‚   â”œâ”€â”€ source_custom_api.md           # Custom API source docs
â”‚   â”œâ”€â”€ destination_snowflake.md       # Snowflake destination docs
â”‚   â”œâ”€â”€ airbyte_Provider.md            # Airbyte provider docs
â”‚   â””â”€â”€ trigger_job.md                 # Job triggering docs
â”‚
â”œâ”€â”€ dev/                                # ğŸ› ï¸ DEVELOPMENT UTILITIES
â”‚   â”œâ”€â”€ csv_checker.py                 # CSV validation tool
â”‚   â””â”€â”€ snowflake-connector/           # Snowflake data upload utilities
â”‚       â””â”€â”€ upload_tables.py           # Bulk table uploader
â”‚
â”œâ”€â”€ example/                            # ğŸ“– USAGE EXAMPLES
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ materials/                          # ğŸ“Š PROJECT MATERIALS
â”‚   â””â”€â”€ elt.svg                        # Diagram assets
â”‚
â””â”€â”€ data/                               # (Not present initially - created by setup)
```

### How Setup Transforms the Structure

The `setup/elt_setup.sh` script performs the following transformations:

#### Step 1: Download Data Archives (via `gdown`)
Downloads three ZIP files to `setup/`:
- `data_api.zip` (~XXX MB) - API source data files
- `data_db.zip` (~XXX MB) - Database source data files  
- `gt.zip` (~XXX MB) - Ground truth validation data

#### Step 2: Extract Archives to `data/`
```bash
unzip data_api.zip -d ../data/source/api     # â†’ data/source/api/
unzip data_db.zip -d ../data/source/db       # â†’ data/source/db/
unzip gt.zip -d ../data/ground_truth         # â†’ data/ground_truth/
```

#### Step 3: Start Docker Infrastructure
```bash
cd ../elt-docker
docker compose up -d
```
Launches containers:
- PostgreSQL (port 5432)
- MongoDB (port 27017)
- LocalStack S3 (port 4566)
- REST API servers (various ports)

#### Step 4: Generate Working Directories (`write_config.py`)
For each of the 100 problems in `elt-bench/`, creates a working copy in `data/inputs/` with:

**Original files (copied from `elt-bench/<problem>/`):**
- `config.yaml` (modified - see below)
- `data_model.yaml`
- `schemas/` directory

**Injected credentials:**
- Updates `config.yaml` with Snowflake account from `setup/destination/snowflake_credential.json`
- Updates `config.yaml` with Airbyte credentials from `setup/airbyte/airbyte_credential.json`
- Creates `snowflake_credential.json` with connection details

**Added resources:**
- `documentation/` (entire directory copied from root)
- `check_job_status.py` (copied from `setup/`)
- `elt/main.tf` (Terraform template copied from `setup/`)

**Resulting structure after setup:**

```
ELT-Bench/
â”œâ”€â”€ elt-bench/                          # ğŸ“¦ ORIGINAL (unchanged, Git-tracked)
â”‚   â”œâ”€â”€ address/
â”‚   â”‚   â”œâ”€â”€ config.yaml                # Template configs (no credentials)
â”‚   â”‚   â”œâ”€â”€ data_model.yaml
â”‚   â”‚   â””â”€â”€ schemas/
â”‚   â””â”€â”€ ... (99 more)
â”‚
â”œâ”€â”€ data/                               # ğŸ¯ GENERATED (gitignored, recreatable)
â”‚   â”‚
â”‚   â”œâ”€â”€ inputs/                         # ğŸ’¼ WORKING COPIES (agents work here)
â”‚   â”‚   â”œâ”€â”€ address/
â”‚   â”‚   â”‚   â”œâ”€â”€ config.yaml            # âœï¸ Modified with credentials
â”‚   â”‚   â”‚   â”œâ”€â”€ data_model.yaml        # Copied from elt-bench/
â”‚   â”‚   â”‚   â”œâ”€â”€ schemas/               # Copied from elt-bench/
â”‚   â”‚   â”‚   â”œâ”€â”€ snowflake_credential.json  # âœ¨ Created (Snowflake auth)
â”‚   â”‚   â”‚   â”œâ”€â”€ documentation/         # ğŸ“š Copied from root
â”‚   â”‚   â”‚   â”œâ”€â”€ check_job_status.py    # ğŸ” Monitoring helper
â”‚   â”‚   â”‚   â””â”€â”€ elt/                   # ğŸ—ï¸ Created directory
â”‚   â”‚   â”‚       â””â”€â”€ main.tf            # Terraform template
â”‚   â”‚   â””â”€â”€ ... (99 more problems)
â”‚   â”‚
â”‚   â”œâ”€â”€ source/                         # ğŸ“¥ SOURCE DATA (extracted from ZIPs)
â”‚   â”‚   â”œâ”€â”€ api/                       # API endpoint data files
â”‚   â”‚   â”‚   â”œâ”€â”€ address/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ *.json, *.csv
â”‚   â”‚   â”‚   â””â”€â”€ ... (problems with API sources)
â”‚   â”‚   â””â”€â”€ db/                        # Database dump files
â”‚   â”‚       â”œâ”€â”€ postgres/
â”‚   â”‚       â”‚   â”œâ”€â”€ address/
â”‚   â”‚       â”‚   â”‚   â””â”€â”€ *.sql, *.csv
â”‚   â”‚       â”‚   â””â”€â”€ ...
â”‚   â”‚       â””â”€â”€ mongodb/
â”‚   â”‚           â””â”€â”€ ...
â”‚   â”‚
â”‚   â”œâ”€â”€ ground_truth/                   # âœ… VALIDATION DATA (extracted from gt.zip)
â”‚   â”‚   â”œâ”€â”€ address/
â”‚   â”‚   â”‚   â””â”€â”€ *.csv                  # Expected output tables
â”‚   â”‚   â””â”€â”€ ... (99 more)
â”‚   â”‚
â”‚   â””â”€â”€ results/                        # ğŸ“Š EVALUATION OUTPUTS (created on eval run)
â”‚       â””â”€â”€ run_YYYYMMDD_HHMMSS/       # Timestamped evaluation run
â”‚           â”œâ”€â”€ eval_address/
â”‚           â”‚   â”œâ”€â”€ stage1.log         # Extraction/loading validation
â”‚           â”‚   â””â”€â”€ stage2.log         # Transformation validation
â”‚           â””â”€â”€ ... (evaluated problems)
â”‚
â”œâ”€â”€ setup/                              # ğŸ”§ (artifacts added after download)
â”‚   â”œâ”€â”€ data_api.zip                   # â¬‡ï¸ Downloaded archive
â”‚   â”œâ”€â”€ data_db.zip                    # â¬‡ï¸ Downloaded archive
â”‚   â”œâ”€â”€ gt.zip                         # â¬‡ï¸ Downloaded archive
â”‚   â””â”€â”€ ... (scripts unchanged)
â”‚
â””â”€â”€ elt-docker/
    â””â”€â”€ rest_api/                       # ğŸŒ (symlinked/copied from data/source/api/)
```

### Directory Roles and Responsibilities

| Directory | Role | Mutability | Git Tracked |
|-----------|------|------------|-------------|
| **`elt-bench/`** | Benchmark definitions (100 problems) | âŒ Read-only | âœ… Yes |
| **`data/inputs/`** | Agent working environment | âœ… Agents modify | âŒ No (gitignored) |
| **`data/source/`** | Source data files (extracted from ZIPs) | âŒ Read-only | âŒ No (gitignored) |
| **`data/ground_truth/`** | Expected outputs for validation | âŒ Read-only | âŒ No (gitignored) |
| **`data/results/`** | Evaluation outputs | âœ… Written by `eva.py` | âŒ No (gitignored) |
| **`setup/`** | Setup scripts & credential templates | ğŸ‘¤ User fills credentials | âœ… Yes (except ZIPs) |
| **`evaluation/`** | Evaluation scripts & SQL queries | âŒ Framework code | âœ… Yes |
| **`elt-docker/`** | Docker infrastructure | âŒ Infrastructure | âœ… Yes |
| **`agents/`** | Agent implementations | ğŸ‘¤ User develops | âœ… Yes |
| **`documentation/`** | API/provider guides | âŒ Reference material | âœ… Yes |
| **`dev/`** | Development utilities | ğŸ‘¤ Helper tools | âœ… Yes |

**Key Principles:**
- **`elt-bench/`** is the source of truth (never modify directly)
- **`data/`** is ephemeral (can be deleted and regenerated)
- **Agents only work in `data/inputs/`** (isolated from originals)
- **Credentials never committed** (stored in `setup/`, injected into `data/inputs/`)

**File Flow Summary:**
```
elt-bench/<problem>/          â†’  (write_config.py)  â†’  data/inputs/<problem>/
â”œâ”€â”€ config.yaml (template)                            â”œâ”€â”€ config.yaml (+ credentials)
â”œâ”€â”€ data_model.yaml                                   â”œâ”€â”€ data_model.yaml
â””â”€â”€ schemas/                                          â”œâ”€â”€ schemas/
                                                      â”œâ”€â”€ snowflake_credential.json (new)
documentation/ (root)         â†’  (copied)         â†’  â”œâ”€â”€ documentation/
setup/check_job_status.py     â†’  (copied)         â†’  â”œâ”€â”€ check_job_status.py
setup/main.tf                 â†’  (copied)         â†’  â””â”€â”€ elt/main.tf

setup/data_*.zip              â†’  (extracted)      â†’  data/source/{api,db}/
setup/gt.zip                  â†’  (extracted)      â†’  data/ground_truth/
```
## Workflow Overview

The complete ELT-Bench workflow consists of three phases:

### Phase 1: Setup (One-time)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. User fills credentials                                   â”‚
â”‚    â”œâ”€â”€ setup/airbyte/airbyte_credential.json               â”‚
â”‚    â””â”€â”€ setup/destination/snowflake_credential.json         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2. Run setup/elt_setup.sh                                   â”‚
â”‚    â”œâ”€â”€ Downloads ZIPs (data_api, data_db, gt)              â”‚
â”‚    â”œâ”€â”€ Extracts to data/ directory                         â”‚
â”‚    â”œâ”€â”€ Starts Docker containers                            â”‚
â”‚    â””â”€â”€ Runs write_config.py                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 3. write_config.py generates data/inputs/                   â”‚
â”‚    â”œâ”€â”€ Copies elt-bench/ â†’ data/inputs/                    â”‚
â”‚    â”œâ”€â”€ Injects credentials into config.yaml                â”‚
â”‚    â”œâ”€â”€ Creates snowflake_credential.json                   â”‚
â”‚    â””â”€â”€ Adds documentation, helpers, templates              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
          data/inputs/ ready for agents
          data/source/ populated with source data
          data/ground_truth/ populated with expected outputs
```

### Phase 2: Agent Execution (Iterative)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Agent operates in: data/inputs/<problem>/                   â”‚
â”‚                                                             â”‚
â”‚ 1. Reads problem requirements                               â”‚
â”‚    â”œâ”€â”€ config.yaml (sources, credentials)                  â”‚
â”‚    â”œâ”€â”€ data_model.yaml (target schema)                     â”‚
â”‚    â””â”€â”€ documentation/ (API guides)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2. Generates ELT pipeline code                              â”‚
â”‚    â”œâ”€â”€ Extract: Airbyte connectors, Python scripts         â”‚
â”‚    â”œâ”€â”€ Load: Snowflake SQL, Terraform                      â”‚
â”‚    â””â”€â”€ Transform: dbt, SQL, Python                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 3. Executes pipeline                                        â”‚
â”‚    â”œâ”€â”€ Triggers data extraction                            â”‚
â”‚    â”œâ”€â”€ Loads to Snowflake                                  â”‚
â”‚    â”œâ”€â”€ Runs transformations                                â”‚
â”‚    â””â”€â”€ Uses check_job_status.py to monitor                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
            Data loaded into Snowflake
```

### Phase 3: Evaluation (Validation)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Run: python evaluation/eva.py --folder <name> --example_index 0-99 â”‚
â”‚                                                             â”‚
â”‚ 1. Stage 1: Extraction/Loading Validation                  â”‚
â”‚    â”œâ”€â”€ Connects to Snowflake                               â”‚
â”‚    â”œâ”€â”€ Queries raw tables                                  â”‚
â”‚    â”œâ”€â”€ Compares vs data/ground_truth/<problem>/*.csv       â”‚
â”‚    â””â”€â”€ Logs to data/results/<folder>/eval_<problem>/stage1.log â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2. Stage 2: Transformation Validation                       â”‚
â”‚    â”œâ”€â”€ Runs SQL queries from evaluation/<problem>/*.sql    â”‚
â”‚    â”œâ”€â”€ Compares results vs ground truth                    â”‚
â”‚    â””â”€â”€ Logs to data/results/<folder>/eval_<problem>/stage2.log â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
        Results saved to data/results/<folder>/
```

### Directory Interaction Map
```
elt-bench/          (read-only templates)
    â†“ copied by write_config.py
data/inputs/        (agent workspace - credentials injected)
    â†“ agent reads
  Agent             (generates code, executes pipeline)
    â†“ writes to
Snowflake           (destination database)
    â†“ queried by
evaluation/         (validation scripts + SQL queries)
    â†“ compares against
data/ground_truth/  (expected outputs)
    â†“ results written to
data/results/       (evaluation logs & scores)
```

## Environment Setup

### Prerequisites

Before starting, ensure you have installed:

| Tool | Purpose | Installation Guide |
|------|---------|-------------------|
| **Docker** | Runs PostgreSQL, MongoDB, S3, API sources | [Install Docker](https://docs.docker.com/get-docker/) |
| **Conda** | Python environment management | [Install Conda](https://docs.conda.io/projects/conda/en/stable/user-guide/install/index.html) |
| **Airbyte** | Data integration platform (optional*) | [Airbyte OSS Quickstart](https://docs.airbyte.com/using-airbyte/getting-started/oss-quickstart) |
| **psql** | PostgreSQL CLI (optional*) | [Install psql](https://www.timescale.com/blog/how-to-install-psql-on-mac-ubuntu-debian-windows) |

**Note:** *Optional components only needed for full Extract-Load workflow. Transform-only workflows can skip Airbyte and psql.

### Setup Steps

#### 1. Install Conda Environment

```bash
# Create and activate conda environment
conda create -y -n elt
conda activate elt
conda install -y python=3.11

# Install Python dependencies
cd setup
pip install -r requirements.txt
```

#### 2. Configure Snowflake (Data Destination)

**2.1. Create Snowflake Resources**

Execute the SQL script in your Snowflake account:

```bash
# Copy contents of setup/destination/setup.sql
# Paste into Snowflake worksheet and run "Run All"
```

This creates:
- Database: `AIRBYTE_DATABASE`
- Warehouse: `AIRBYTE_WAREHOUSE`
- Role: `AIRBYTE_ROLE`
- User: `AIRBYTE_USER` (password: `Snowflake@123`)
- Schema: `AIRBYTE_SCHEMA`

**2.2. Fill Snowflake Credentials**

Edit `setup/destination/snowflake_credential.json`:

```json
{
  "account": "your-account.region.snowflakecomputing.com",
  "user": "AIRBYTE_USER",
  "password": "Snowflake@123"
}
```

**Important:** Use role `AIRBYTE_ROLE`, **not** `SYSADMIN`.

#### 3. Configure Airbyte (Optional - for EL stages)

**3.1. Deploy Airbyte**

```bash
# May require sudo depending on your setup
abctl local install

# Retrieve credentials
abctl local credentials
```

**3.2. Import Custom Connector**

1. Navigate to [http://localhost:8000/](http://localhost:8000/)
2. Login with credentials from step 3.1
3. Go to **Builder > Import a YAML**
4. Upload `setup/elt_bench.yaml`
5. Click **Publish** â†’ type "ignore warnings" â†’ Publish to workspace

**3.3. Get Workspace & Definition IDs**

1. Go to **Sources > Custom > ELT Bench**
2. Extract IDs from URL:
   ```
   http://localhost:8012/workspaces/<workspace_id>/source/new-source/<api_definition_id>
   ```

**3.4. Fill Airbyte Credentials**

Edit `setup/airbyte/airbyte_credential.json`:

```json
{
  "username": "your-username",
  "password": "your-password",
  "workspace_id": "your-workspace-id",
  "api_definition_id": "your-api-definition-id"
}
```

#### 4. Run Main Setup Script

Execute the setup script to:
- Download source data and ground truth (via `gdown`)
- Extract to `data/` directory
- Start Docker containers
- Generate `data/inputs/` with credentials

```bash
cd setup
bash elt_setup.sh
```

**What this does:**
1. Downloads three ZIP files:
   - `data_api.zip` - API source data
   - `data_db.zip` - Database source data
   - `gt.zip` - Ground truth validation data
2. Extracts archives to `data/source/` and `data/ground_truth/`
3. Starts Docker containers (PostgreSQL, MongoDB, LocalStack S3, APIs)
4. Runs `write_config.py` to generate `data/inputs/` with injected credentials

**Expected output structure:**
```
data/
â”œâ”€â”€ inputs/           # 100 problem folders with credentials
â”œâ”€â”€ source/
â”‚   â”œâ”€â”€ api/         # API data files
â”‚   â””â”€â”€ db/          # Database dumps
â””â”€â”€ ground_truth/    # Expected outputs (100 folders)
```

#### 5. Load Data to Snowflake

Upload benchmark data to Snowflake:

```bash
# From project root
python3 dev/snowflake-connector/upload_tables.py --example_index 0-99
```

**Parameters:**
- `--example_index 0-99`: Uploads all 100 problems (inclusive range)
- `--example_index 0-4`: Uploads only problems 0-4
- `--example_index 2,5,7`: Uploads specific problems

**Verification:**
After upload, check Snowflake for tables in `AIRBYTE_DATABASE.AIRBYTE_SCHEMA`.

### Optional: Database-Specific Setup (for EL stages)

If working with Extract-Load stages, you may need to populate source databases:

```bash
cd setup

# Initialize PostgreSQL tables
bash data_setup.sh $(pwd)

# Load MongoDB data
python3 mongo.py --path $(pwd)
```

### Troubleshooting Setup

| Issue | Solution |
|-------|----------|
| `gdown` fails to download | Check internet connection, Google Drive quota |
| Docker containers fail to start | Check port availability (5432, 27017, 4566) |
| Snowflake connection fails | Verify credentials in `setup/destination/snowflake_credential.json` |
| `data/inputs/` empty after setup | Run `python3 setup/write_config.py` manually |
| Airbyte UI not accessible | Ensure Airbyte is running: `abctl local status` |

## Running Agents

Agents work within the `data/inputs/<problem>/` directory structure. Each problem folder contains everything an agent needs to build and execute an ELT pipeline.

### Agent Input Structure

For each problem in `data/inputs/<problem>/`:

```
data/inputs/address/
â”œâ”€â”€ config.yaml                 # Data source configurations (with credentials)
â”œâ”€â”€ data_model.yaml             # Required output schema definition
â”œâ”€â”€ schemas/                    # Source data schemas
â”‚   â”œâ”€â”€ postgres_schema.json
â”‚   â”œâ”€â”€ mongodb_schema.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ snowflake_credential.json   # Snowflake connection details
â”œâ”€â”€ documentation/              # API/provider documentation
â”‚   â”œâ”€â”€ source_postgres.md
â”‚   â”œâ”€â”€ source_mongodb_v2.md
â”‚   â”œâ”€â”€ destination_snowflake.md
â”‚   â””â”€â”€ ...
â”œâ”€â”€ check_job_status.py         # Helper to monitor pipeline execution
â””â”€â”€ elt/                        # Agent's working directory
    â””â”€â”€ main.tf                 # Terraform template (if needed)
```

### Agent Workflow

1. **Read Problem Requirements**
   - `config.yaml`: Identifies data sources (PostgreSQL, MongoDB, S3, APIs, files)
   - `data_model.yaml`: Defines target schema and transformations
   - `schemas/`: Provides source data structures

2. **Generate Pipeline Code**
   - **Extract**: Create Airbyte connections, API calls, or file readers
   - **Load**: Write Snowflake DDL, configure destinations
   - **Transform**: Generate SQL transformations, dbt models, or Python scripts

3. **Execute Pipeline**
   - Deploy infrastructure (Terraform, Airbyte connectors)
   - Trigger data extraction and loading
   - Run transformations
   - Use `check_job_status.py` to monitor progress

4. **Output to Snowflake**
   - Tables loaded to: `AIRBYTE_DATABASE.AIRBYTE_SCHEMA.<table_name>`
   - Agent should ensure data matches `data_model.yaml` specifications

### Agent Implementations

See the `agents/` directory for example implementations:

- **`agents/spider-agent/`**: Spider-based agent implementation
- **`agents/SWE-agent/`**: SWE-agent implementation

Each agent directory contains:
- Setup instructions
- Configuration files
- Execution scripts
- Documentation

### Development Utilities

Agents can use the following utilities:

| Utility | Location | Purpose |
|---------|----------|---------|
| `check_job_status.py` | Each `data/inputs/<problem>/` | Monitor Airbyte job status |
| `upload_tables.py` | `dev/snowflake-connector/` | Upload data to Snowflake |
| `csv_checker.py` | `dev/` | Validate CSV outputs |

### Agent Best Practices

1. **Never modify `elt-bench/`** - Always work in `data/inputs/`
2. **Use provided credentials** - Read from `config.yaml` and `snowflake_credential.json`
3. **Follow schema specifications** - Match output to `data_model.yaml`
4. **Log progress** - Use `check_job_status.py` for monitoring
5. **Handle errors gracefully** - Implement retry logic for transient failures

## Evaluation

The evaluation framework validates agent performance across two stages:
- **Stage 1**: Data extraction and loading (EL) - verifies raw data in Snowflake
- **Stage 2**: Data transformation (T) - validates transformed output against expected results

### Running Evaluation

```bash
cd evaluation
python eva.py --folder <folder_name> --example_index <range>
```

**Parameters:**

| Parameter | Description | Examples |
|-----------|-------------|----------|
| `--folder` | Name for this evaluation run | `spider_run_1`, `my_agent_test` |
| `--example_index` | Problems to evaluate | `0-99` (all), `0-4` (range), `2,5,7` (specific) |

**Examples:**

```bash
# Evaluate all 100 problems
python eva.py --folder full_evaluation --example_index 0-99

# Evaluate first 5 problems for testing
python eva.py --folder quick_test --example_index 0-4

# Evaluate specific problems
python eva.py --folder targeted_test --example_index 10,25,50,75
```

### Evaluation Process

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 1: Extraction & Loading Validation (eva_stage1.py)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Connects to Snowflake                                     â”‚
â”‚ 2. Queries raw tables in AIRBYTE_DATABASE.AIRBYTE_SCHEMA    â”‚
â”‚ 3. Compares against: data/ground_truth/<problem>/*.csv      â”‚
â”‚ 4. Validates:                                                â”‚
â”‚    â”œâ”€â”€ Row counts match                                      â”‚
â”‚    â”œâ”€â”€ Column names match                                    â”‚
â”‚    â”œâ”€â”€ Data types correct                                    â”‚
â”‚    â””â”€â”€ Values match expected data                           â”‚
â”‚ 5. Logs results to: data/results/<folder>/eval_<problem>/stage1.log â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 2: Transformation Validation (eva_stage2.py)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Reads SQL queries from: evaluation/<problem>/*.sql       â”‚
â”‚ 2. Executes queries against Snowflake                       â”‚
â”‚ 3. Compares results against ground truth                    â”‚
â”‚ 4. Validates:                                                â”‚
â”‚    â”œâ”€â”€ Transformation logic correct                         â”‚
â”‚    â”œâ”€â”€ Aggregations accurate                                â”‚
â”‚    â”œâ”€â”€ Joins produce expected results                       â”‚
â”‚    â””â”€â”€ Final schema matches data_model.yaml                 â”‚
â”‚ 5. Logs results to: data/results/<folder>/eval_<problem>/stage2.log â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Evaluation Output

Results are saved to `data/results/<folder>/`:

```
data/results/
â””â”€â”€ <folder_name>/              # Your evaluation run
    â”œâ”€â”€ eval_address/           # Results for "address" problem
    â”‚   â”œâ”€â”€ stage1.log          # EL validation logs
    â”‚   â””â”€â”€ stage2.log          # Transform validation logs
    â”œâ”€â”€ eval_airline/           # Results for "airline" problem
    â”‚   â”œâ”€â”€ stage1.log
    â”‚   â””â”€â”€ stage2.log
    â””â”€â”€ ... (for each evaluated problem)
```

**Log Format:**

Each log contains:
- **Timestamp**: When evaluation ran
- **Problem Name**: Which problem was evaluated
- **Stage**: Stage 1 or Stage 2
- **Status**: PASS/FAIL
- **Details**: 
  - Row count comparisons
  - Schema validations
  - Data mismatches (if any)
  - Error messages (if failed)

### Evaluation Requirements

Before running evaluation, ensure:

1. **Ground truth exists**: `data/ground_truth/` populated by `setup/elt_setup.sh`
2. **SQL queries exist**: `evaluation/<problem>/*.sql` present
3. **Snowflake has data**: Agent successfully loaded data
4. **Credentials configured**: Snowflake connection works

### Understanding Results

**Stage 1 Pass Criteria:**
- All expected tables exist in Snowflake
- Row counts match ground truth
- Column names and types match
- Data values match ground truth CSVs

**Stage 2 Pass Criteria:**
- Transformation queries execute successfully
- Results match expected output
- Data model requirements satisfied
- All business logic correctly implemented

**Common Failure Reasons:**

| Failure | Cause | Solution |
|---------|-------|----------|
| Stage 1: Table not found | Agent didn't load data | Check agent logs, verify pipeline ran |
| Stage 1: Row count mismatch | Incomplete extraction | Verify source data availability |
| Stage 1: Schema mismatch | Wrong column types | Check `schemas/` and fix data types |
| Stage 2: Query error | Transformation logic wrong | Review `data_model.yaml`, fix SQL |
| Stage 2: Result mismatch | Incorrect aggregation | Debug transformation code |

### Automated Timestamping

**Note:** Evaluation runs are automatically timestamped to prevent overwriting previous results. You can specify a custom folder name with `--folder` for organization.

## Common Issues and Solutions

### Setup Issues

| Issue | Symptoms | Solution |
|-------|----------|----------|
| **Missing `data/inputs/`** | Agent can't find problem folders | Run `cd setup && bash elt_setup.sh` |
| **Empty `data/inputs/`** | Folders exist but no files | Run `cd setup && python3 write_config.py` |
| **Missing ground truth** | Evaluation fails: "ground truth not found" | Re-run `elt_setup.sh` to extract `gt.zip` |
| **Download fails** | `gdown` errors | Check internet, Google Drive quota, retry |
| **Docker containers fail** | Can't connect to PostgreSQL/MongoDB | Check ports 5432, 27017, 4566 not in use |

### Credential Issues

| Issue | Symptoms | Solution |
|-------|----------|----------|
| **Snowflake connection fails** | "Authentication failed" | Verify `setup/destination/snowflake_credential.json` |
| **Wrong Snowflake role** | Permission denied errors | Use `AIRBYTE_ROLE`, not `SYSADMIN` |
| **Airbyte auth fails** | Can't connect to Airbyte API | Verify `setup/airbyte/airbyte_credential.json` |
| **Credentials not in `data/inputs/`** | Agents can't connect | Re-run `python3 setup/write_config.py` |

### Data Issues

| Issue | Symptoms | Solution |
|-------|----------|----------|
| **Tables not in Snowflake** | Evaluation fails: "table not found" | Run `python3 dev/snowflake-connector/upload_tables.py --example_index 0-99` |
| **Partial data in Snowflake** | Row count mismatches | Check source data in `data/source/`, re-upload |
| **Source database empty** | No data to extract | Run `cd setup && bash data_setup.sh $(pwd)` |
| **MongoDB data missing** | MongoDB queries return no results | Run `cd setup && python3 mongo.py --path $(pwd)` |

### Evaluation Issues

| Issue | Symptoms | Solution |
|-------|----------|----------|
| **"Ground truth not found"** | Can't find CSV files | Ensure `data/ground_truth/` populated by setup |
| **SQL query files missing** | Stage 2 fails | Check `evaluation/<problem>/*.sql` exists |
| **Stage 1 passes, Stage 2 fails** | Data loaded but transformations wrong | Review agent's transformation logic |
| **Results overwritten** | Previous results lost | Use unique `--folder` names or rely on auto-timestamping |

### Directory Confusion

**Problem:** Unclear whether to work in `elt-bench/` or `data/inputs/`

**Solution:**
- **`elt-bench/`**: Source of truth, **never modify** directly
- **`data/inputs/`**: Working copies for agents, **safe to modify**
- To reset: Delete `data/inputs/` and re-run `setup/write_config.py`

### Reset Workflows

**Reset generated data (keep credentials):**
```bash
rm -rf data/inputs/
cd setup
python3 write_config.py
```

**Reset evaluation results:**
```bash
rm -rf data/results/*
```

**Complete reset (re-download everything):**
```bash
rm -rf data/
cd setup
bash elt_setup.sh
```

**Reset Snowflake data:**
```sql
-- In Snowflake worksheet
USE DATABASE AIRBYTE_DATABASE;
USE SCHEMA AIRBYTE_SCHEMA;
DROP SCHEMA AIRBYTE_SCHEMA CASCADE;
CREATE SCHEMA AIRBYTE_SCHEMA;
```

Then re-upload:
```bash
python3 dev/snowflake-connector/upload_tables.py --example_index 0-99
```

## Project Maintenance

### Regenerating `data/inputs/` Directory

If credentials change or you need fresh working copies:

```bash
cd setup
python3 write_config.py
```

**What happens:**
1. Deletes existing `data/inputs/` (if present)
2. Copies all 100 problems from `elt-bench/` â†’ `data/inputs/`
3. Injects current credentials from:
   - `setup/destination/snowflake_credential.json`
   - `setup/airbyte/airbyte_credential.json`
4. Creates `snowflake_credential.json` in each problem folder
5. Copies `documentation/`, `check_job_status.py`, `elt/main.tf`

**Use cases:**
- Updated Snowflake credentials
- Updated Airbyte workspace/definition IDs
- Corrupted `data/inputs/` needs reset
- Want to test with fresh environment

### Updating Credentials

**Step 1: Update credential files**

```bash
# Edit Snowflake credentials
vim setup/destination/snowflake_credential.json

# Edit Airbyte credentials
vim setup/airbyte/airbyte_credential.json
```

**Step 2: Propagate to working directories**

```bash
cd setup
python3 write_config.py
```

All 100 problem folders in `data/inputs/` will receive updated credentials.

### Cleaning Evaluation Results

**Remove all evaluation runs:**
```bash
rm -rf data/results/*
```

**Remove specific run:**
```bash
rm -rf data/results/my_old_run/
```

**Archive before cleaning:**
```bash
# Create archive
tar -czf evaluation_archive_$(date +%Y%m%d).tar.gz data/results/

# Then clean
rm -rf data/results/*
```

### Resetting to Fresh State

**Option 1: Full reset (everything)**
```bash
# Remove all generated data
rm -rf data/

# Re-run complete setup
cd setup
bash elt_setup.sh

# Re-upload to Snowflake
cd ..
python3 dev/snowflake-connector/upload_tables.py --example_index 0-99
```

**Option 2: Partial reset (keep downloaded data)**
```bash
# Remove only working directories and results
rm -rf data/inputs/ data/results/

# Regenerate working directories
cd setup
python3 write_config.py
```

**Option 3: Reset only evaluation**
```bash
rm -rf data/results/*
```

### Version Control Best Practices

**What to commit:**
- âœ… `elt-bench/` (benchmark definitions)
- âœ… `setup/*.py`, `setup/*.sh` (setup scripts)
- âœ… `evaluation/` (evaluation framework)
- âœ… `agents/` (agent implementations)
- âœ… `documentation/` (API docs)
- âœ… `dev/` (development utilities)

**What to ignore (already in `.gitignore`):**
- âŒ `data/` (all generated content)
- âŒ `setup/*.zip` (downloaded archives)
- âŒ `setup/*_credential.json` (credentials)
- âŒ `__pycache__/` (Python cache)
- âŒ `*.pyc` (compiled Python)

### Maintaining Data Integrity

**Verify ground truth integrity:**
```bash
# Check if ground truth exists for all problems
for i in {0..99}; do
  problem=$(ls -d elt-bench/* | sed -n "$((i+1))p" | xargs basename)
  if [ ! -d "data/ground_truth/$problem" ]; then
    echo "Missing: $problem"
  fi
done
```

**Verify `data/inputs/` completeness:**
```bash
# Should show 100 directories
ls -d data/inputs/*/ | wc -l

# Check each has required files
for dir in data/inputs/*/; do
  if [ ! -f "$dir/config.yaml" ] || [ ! -f "$dir/snowflake_credential.json" ]; then
    echo "Incomplete: $dir"
  fi
done
```

**Verify Snowflake tables:**
```sql
-- In Snowflake worksheet
USE DATABASE AIRBYTE_DATABASE;
USE SCHEMA AIRBYTE_SCHEMA;

-- Count tables (should have tables for loaded problems)
SHOW TABLES;

-- Check specific table
SELECT COUNT(*) FROM address_table;
```

### Adding New Problems

To add a new benchmark problem:

1. **Create problem folder in `elt-bench/`:**
   ```bash
   mkdir elt-bench/new_problem
   ```

2. **Add required files:**
   ```
   elt-bench/new_problem/
   â”œâ”€â”€ config.yaml          # Source configurations
   â”œâ”€â”€ data_model.yaml      # Target schema
   â””â”€â”€ schemas/             # Source schemas
   ```

3. **Create evaluation queries:**
   ```bash
   mkdir evaluation/new_problem
   # Add *.sql files with expected queries
   ```

4. **Add ground truth:**
   ```bash
   mkdir data/ground_truth/new_problem
   # Add expected output CSVs
   ```

5. **Regenerate working directory:**
   ```bash
   cd setup
   python3 write_config.py
   ```

### Monitoring Disk Usage

The `data/` directory can grow large:

```bash
# Check total size
du -sh data/

# Check breakdown
du -sh data/*/

# Largest problem folders
du -sh data/inputs/* | sort -h | tail -20
```

**Space-saving tips:**
- Delete `data/results/` after archiving
- Keep only recent evaluation runs
- Compress old archives: `tar -czf old_results.tar.gz data/results/old_run/`

---

## Quick Reference

### Essential Commands

**Initial Setup:**
```bash
# 1. Create conda environment
conda create -y -n elt && conda activate elt && conda install -y python=3.11

# 2. Install dependencies
cd setup && pip install -r requirements.txt

# 3. Fill credentials (edit these files)
vim setup/destination/snowflake_credential.json
vim setup/airbyte/airbyte_credential.json

# 4. Run setup
bash elt_setup.sh

# 5. Upload to Snowflake
cd .. && python3 dev/snowflake-connector/upload_tables.py --example_index 0-99
```

**Run Evaluation:**
```bash
cd evaluation
python eva.py --folder <run_name> --example_index 0-99
```

**Regenerate Working Directories:**
```bash
cd setup
python3 write_config.py
```

### Key File Locations

| What You Need | Where to Find It |
|---------------|------------------|
| **Problem definitions** | `elt-bench/<problem>/` |
| **Agent workspace** | `data/inputs/<problem>/` |
| **Source data** | `data/source/api/` and `data/source/db/` |
| **Ground truth** | `data/ground_truth/<problem>/` |
| **Evaluation results** | `data/results/<folder>/` |
| **Snowflake credentials** | `setup/destination/snowflake_credential.json` |
| **Airbyte credentials** | `setup/airbyte/airbyte_credential.json` |
| **SQL validation queries** | `evaluation/<problem>/*.sql` |

### Directory Quick Guide

```
elt-bench/          â† Original problems (DON'T MODIFY)
data/inputs/        â† Agent workspace (AGENTS WORK HERE)
data/ground_truth/  â† Expected outputs (FOR VALIDATION)
data/results/       â† Evaluation logs (GENERATED)
setup/              â† Setup scripts + credentials (USER FILLS)
evaluation/         â† Validation framework (RUNS EVALUATION)
```

### Common Workflows

**Test agent on one problem:**
```bash
# Run agent on data/inputs/address/
<your_agent_command>

# Evaluate
cd evaluation
python eva.py --folder test_run --example_index 0
```

**Full benchmark run:**
```bash
# Run agent on all 100 problems
<your_agent_loop_command>

# Evaluate all
cd evaluation
python eva.py --folder full_run --example_index 0-99
```

**Reset everything:**
```bash
rm -rf data/
cd setup && bash elt_setup.sh
cd .. && python3 dev/snowflake-connector/upload_tables.py --example_index 0-99
```

### Support & Documentation

- **API Documentation**: See `documentation/` folder
- **Agent Examples**: See `agents/` folder  
- **Setup Issues**: See [Common Issues](#common-issues-and-solutions) section
- **Evaluation Details**: See [Evaluation](#evaluation) section

---

**Built with â¤ï¸ for AI agent ELT pipeline benchmarking**