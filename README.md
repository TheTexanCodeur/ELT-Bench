# ELT-Bench (Transform Stage Only)

A benchmark suite for evaluating AI agents on **data transformation tasks** using Snowflake and SQL/DBT.

**Based on:** [uiuc-kang-lab/ELT-Bench](https://github.com/uiuc-kang-lab/ELT-Bench)  
**Modification:** This fork focuses **exclusively on the Transform (T)** stage. All Extract (E) and Load (L) operations are pre-completed.

## ğŸ¯ What is ELT-Bench?

ELT-Bench contains **100 data transformation problems** where AI agents must:
1. Read source tables from Snowflake (`AIRBYTE_SCHEMA`)
2. Generate SQL/DBT transformations based on requirements
3. Create target tables in the same schema

**Important:** This benchmark evaluates **only the Transform (T)** stage. Source data is pre-loaded into Snowflake before agents start working. Unlike the original ELT-Bench, agents do **not** need to configure or run Airbyte for data extraction and loading.

---

## ğŸš€ Quick Start

### 1. Install Prerequisites

- **Docker** - For running agent containers
- **Conda** - For Python environment management
- **Snowflake Account** - For data storage and transformations

### 2. Setup Environment

```bash
# Clone repository
git clone <repo-url>
cd ELT-Bench

# Create conda environment
conda create -y -n elt python=3.11
conda activate elt

# Install dependencies
cd setup
pip install -r requirements.txt
```

### 3. Configure Snowflake

**A. Run the setup SQL in your Snowflake account:**

Copy and execute `setup/destination/setup.sql` in Snowflake. This creates:
- Database: `AIRBYTE_DATABASE`
- Schema: `AIRBYTE_SCHEMA`
- User: `AIRBYTE_USER` (password: `Snowflake@123`)
- Role: `AIRBYTE_ROLE`
- Warehouse: `AIRBYTE_WAREHOUSE`

**B. Add your Snowflake credentials:**

Edit `setup/destination/snowflake_credential.json`:
```json
{
  "account": "your-account.region.snowflakecomputing.com",
  "user": "AIRBYTE_USER",
  "password": "Snowflake@123"
}
```

âš ï¸ **Important:** Use role `AIRBYTE_ROLE`, not `SYSADMIN`

### 4. Download and Setup Data

```bash
# From setup/ directory
bash elt_setup.sh
```

This will:
- Download source data, ground truth, and schemas (via Google Drive)
- Extract files to `data/` directory
- Generate agent workspaces in `data/inputs/` with credentials

### 5. Load Source Data to Snowflake

```bash
# From project root
python3 dev/snowflake-connector/upload_tables.py --example_index 0-99
```

This uploads all 100 problems' source tables to `AIRBYTE_SCHEMA`. You can also upload specific problems:
- `--example_index 0-4` - Upload first 5 problems
- `--example_index 0,5,10` - Upload specific problems

**Verify:** Check Snowflake for tables in `AIRBYTE_DATABASE.AIRBYTE_SCHEMA`

---

## ğŸ¤– Running Agents

### Agent Workspace Structure

Each problem has a workspace in `data/inputs/<problem>/`:

```
data/inputs/address/
â”œâ”€â”€ config.yaml                 # Source table information
â”œâ”€â”€ data_model.yaml             # Target schema requirements
â”œâ”€â”€ schemas/                    # Source data schemas
â”‚   â””â”€â”€ *.json
â””â”€â”€ snowflake_credential.json   # Snowflake connection info
```

### Agent Requirements

Your agent should:

1. **Read requirements** from the workspace files
2. **Generate transformation code** (SQL, DBT, Python)
3. **Execute transformations** that:
   - Read from `AIRBYTE_SCHEMA` (source tables)
   - Write to `AIRBYTE_SCHEMA` (target tables)
   - Match the schema defined in `data_model.yaml`

### Example Agent Implementations

See these for reference:
- `agents/spider-agent/` - Spider-based agent
---

## âœ… Evaluation

### Run Evaluation

```bash
cd evaluation
python eva.py --folder <run_name> --example_index <range>
```

**Examples:**
```bash
# Evaluate all 100 problems
python eva.py --folder my_run --example_index 0-99

# Evaluate first 5 problems (for testing)
python eva.py --folder test --example_index 0-4

# Evaluate specific problems
python eva.py --folder targeted --example_index 10,25,50
```

### Evaluation Stages

**Stage 1: Source Table Validation**
- Verifies source tables exist in `AIRBYTE_SCHEMA`
- Compares against ground truth data
- Validates row counts, schemas, and data values

**Stage 2: Transformation Validation**
- Executes SQL queries from `evaluation/<problem>/*.sql`
- Compares transformation results against expected output
- Validates business logic and data model compliance

### Results

Results are saved to `data/results/<run_name>/`:

```
data/results/my_run/
â”œâ”€â”€ eval_address/
â”‚   â”œâ”€â”€ stage1.log    # Source validation logs
â”‚   â””â”€â”€ stage2.log    # Transformation validation logs
â”œâ”€â”€ eval_airline/
â”‚   â”œâ”€â”€ stage1.log
â”‚   â””â”€â”€ stage2.log
â””â”€â”€ ...
```

---

## ğŸ“ Project Structure

### Directory Layout (Before Setup)

```
ELT-Bench/
â”œâ”€â”€ README.md                           # Project documentation
â”‚
â”œâ”€â”€ elt-bench/                          # ğŸ“¦ BENCHMARK DEFINITIONS (100 problems)
â”‚   â”œâ”€â”€ address/                       # Problem 1: Address data
â”‚   â”‚   â”œâ”€â”€ config.yaml                # Data source configurations
â”‚   â”‚   â”œâ”€â”€ data_model.yaml            # Required output schema
â”‚   â”‚   â””â”€â”€ schemas/                   # Source data schemas
â”‚   â”œâ”€â”€ airline/                       # Problem 2: Airline data
â”‚   â””â”€â”€ ... (98 more problems)         # Problems 3-100
â”‚
â”œâ”€â”€ setup/                              # ğŸ”§ SETUP SCRIPTS & CREDENTIALS
â”‚   â”œâ”€â”€ elt_setup.sh                   # Main setup orchestrator
â”‚   â”œâ”€â”€ write_config.py                # Generates data/inputs/ from elt-bench/
â”‚   â”œâ”€â”€ requirements.txt               # Python dependencies
â”‚   â”‚
â”‚   â””â”€â”€ destination/                   # Snowflake credentials (user-filled)
â”‚       â”œâ”€â”€ snowflake_credential.json  # {account, user, password}
â”‚       â””â”€â”€ setup.sql                  # Snowflake setup SQL
â”‚
â”œâ”€â”€ elt-docker/                         # ğŸ³ DOCKER INFRASTRUCTURE (for agent containers)
â”‚   â””â”€â”€ docker-compose.yml             # Defines agent environment images
â”‚
â”œâ”€â”€ evaluation/                         # âœ… EVALUATION FRAMEWORK
â”‚   â”œâ”€â”€ eva.py                         # Main evaluation orchestrator
â”‚   â”œâ”€â”€ eva_stage1.py                  # Stage 1: Source table validation
â”‚   â”œâ”€â”€ eva_stage2.py                  # Stage 2: Transformation validation
â”‚   â”œâ”€â”€ table.json                     # Table metadata for validation
â”‚   â”‚
â”‚   â””â”€â”€ address/, airline/, ... (100)  # Expected SQL queries per problem
â”‚       â””â”€â”€ *.sql                      # Query definitions for comparison
â”‚
â”œâ”€â”€ agents/                             # ğŸ¤– AGENT IMPLEMENTATIONS
â”‚   â”œâ”€â”€ spider-agent/                  # Spider Agent implementation
â”‚   â””â”€â”€ SWE-agent/                     # SWE Agent implementation
â”‚
â”œâ”€â”€ dev/                                # ğŸ› ï¸ DEVELOPMENT UTILITIES
â”‚   â”œâ”€â”€ csv_checker.py                 # CSV validation tool
â”‚   â””â”€â”€ snowflake-connector/           # Snowflake data upload utilities
â”‚       â””â”€â”€ upload_tables.py           # Bulk table uploader
â”‚
â”œâ”€â”€ example/                            # ğŸ“– USAGE EXAMPLES
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ materials/                          # ğŸ“Š PROJECT MATERIALS
â”‚   â””â”€â”€ elt.svg                        # Diagram assets
â”‚
â””â”€â”€ data/                               # (Not present initially - created by setup)
```

### How Setup Transforms the Structure

The `setup/elt_setup.sh` script performs the following transformations:

#### Step 1: Download Data Archives (via `gdown`)
Downloads three ZIP files to `setup/`:
- `data_api.zip` (~XXX MB) - API source data files
- `data_db.zip` (~XXX MB) - Database source data files  
- `gt.zip` (~XXX MB) - Ground truth validation data

#### Step 2: Extract Archives to `data/`
```bash
unzip data_api.zip -d ../data/source/api     # â†’ data/source/api/
unzip data_db.zip -d ../data/source/db       # â†’ data/source/db/
unzip gt.zip -d ../data/ground_truth         # â†’ data/ground_truth/
```

#### Step 3: Generate Working Directories (`write_config.py`)
For each of the 100 problems in `elt-bench/`, creates a working copy in `data/inputs/` with:

**Original files (copied from `elt-bench/<problem>/`):**
- `config.yaml`
- `data_model.yaml`
- `schemas/` directory

**Injected credentials:**
- Creates `snowflake_credential.json` with connection details from `setup/destination/snowflake_credential.json`

**Resulting structure after setup:**

```
ELT-Bench/
â”œâ”€â”€ elt-bench/                          # ğŸ“¦ ORIGINAL (unchanged, Git-tracked)
â”‚   â”œâ”€â”€ address/
â”‚   â”‚   â”œâ”€â”€ config.yaml                # Template configs (no credentials)
â”‚   â”‚   â”œâ”€â”€ data_model.yaml
â”‚   â”‚   â””â”€â”€ schemas/
â”‚   â””â”€â”€ ... (99 more)
â”‚
â”œâ”€â”€ data/                               # ğŸ¯ GENERATED (gitignored, recreatable)
â”‚   â”‚
â”‚   â”œâ”€â”€ inputs/                         # ğŸ’¼ WORKING COPIES (agents work here)
â”‚   â”‚   â”œâ”€â”€ address/
â”‚   â”‚   â”‚   â”œâ”€â”€ config.yaml            # Copied from elt-bench/
â”‚   â”‚   â”‚   â”œâ”€â”€ data_model.yaml        # Copied from elt-bench/
â”‚   â”‚   â”‚   â”œâ”€â”€ schemas/               # Copied from elt-bench/
â”‚   â”‚   â”‚   â””â”€â”€ snowflake_credential.json  # âœ¨ Created (Snowflake auth)
â”‚   â”‚   â””â”€â”€ ... (99 more problems)
â”‚   â”‚
â”‚   â”œâ”€â”€ source/                         # ğŸ“¥ SOURCE DATA (extracted from ZIPs)
â”‚   â”‚   â”œâ”€â”€ api/                       # API endpoint data files
â”‚   â”‚   â”‚   â”œâ”€â”€ address/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ *.json, *.csv
â”‚   â”‚   â”‚   â””â”€â”€ ... (problems with API sources)
â”‚   â”‚   â””â”€â”€ db/                        # Database dump files
â”‚   â”‚       â”œâ”€â”€ postgres/
â”‚   â”‚       â”‚   â”œâ”€â”€ address/
â”‚   â”‚       â”‚   â”‚   â””â”€â”€ *.sql, *.csv
â”‚   â”‚       â”‚   â””â”€â”€ ...
â”‚   â”‚       â””â”€â”€ mongodb/
â”‚   â”‚           â””â”€â”€ ...
â”‚   â”‚
â”‚   â”œâ”€â”€ ground_truth/                   # âœ… VALIDATION DATA (extracted from gt.zip)
â”‚   â”‚   â”œâ”€â”€ address/
â”‚   â”‚   â”‚   â””â”€â”€ *.csv                  # Expected output tables
â”‚   â”‚   â””â”€â”€ ... (99 more)
â”‚   â”‚
â”‚   â””â”€â”€ results/                        # ğŸ“Š EVALUATION OUTPUTS (created on eval run)
â”‚       â””â”€â”€ run_YYYYMMDD_HHMMSS/       # Timestamped evaluation run
â”‚           â”œâ”€â”€ eval_address/
â”‚           â”‚   â”œâ”€â”€ stage1.log         # Source table validation
â”‚           â”‚   â””â”€â”€ stage2.log         # Transformation validation
â”‚           â””â”€â”€ ... (evaluated problems)
â”‚
â”œâ”€â”€ setup/                              # ğŸ”§ (artifacts added after download)
â”‚   â”œâ”€â”€ data_api.zip                   # â¬‡ï¸ Downloaded archive
â”‚   â”œâ”€â”€ data_db.zip                    # â¬‡ï¸ Downloaded archive
â”‚   â”œâ”€â”€ gt.zip                         # â¬‡ï¸ Downloaded archive
â”‚   â””â”€â”€ ... (scripts unchanged)
â”‚
â””â”€â”€ elt-docker/
    â””â”€â”€ (agent docker images)
```

### Directory Roles and Responsibilities

| Directory | Role | Mutability | Git Tracked |
|-----------|------|------------|-------------|
| **`elt-bench/`** | Benchmark definitions (100 problems) | âŒ Read-only | âœ… Yes |
| **`data/inputs/`** | Agent working environment | âœ… Agents modify | âŒ No (gitignored) |
| **`data/source/`** | Source data files (extracted from ZIPs) | âŒ Read-only | âŒ No (gitignored) |
| **`data/ground_truth/`** | Expected outputs for validation | âŒ Read-only | âŒ No (gitignored) |
| **`data/results/`** | Evaluation outputs | âœ… Written by `eva.py` | âŒ No (gitignored) |
| **`setup/`** | Setup scripts & credential templates | ğŸ‘¤ User fills credentials | âœ… Yes (except ZIPs) |
| **`evaluation/`** | Evaluation scripts & SQL queries | âŒ Framework code | âœ… Yes |
| **`elt-docker/`** | Agent container images | âŒ Infrastructure | âœ… Yes |
| **`agents/`** | Agent implementations | ğŸ‘¤ User develops | âœ… Yes |
| **`dev/`** | Development utilities | ğŸ‘¤ Helper tools | âœ… Yes |

**Key Principles:**
- **`elt-bench/`** is the source of truth (never modify directly)
- **`data/`** is ephemeral (can be deleted and regenerated)
- **Agents only work in `data/inputs/`** (isolated from originals)
- **Credentials never committed** (stored in `setup/`, injected into `data/inputs/`)

**File Flow Summary:**
```
elt-bench/<problem>/          â†’  (write_config.py)  â†’  data/inputs/<problem>/
â”œâ”€â”€ config.yaml                                       â”œâ”€â”€ config.yaml
â”œâ”€â”€ data_model.yaml                                   â”œâ”€â”€ data_model.yaml
â””â”€â”€ schemas/                                          â”œâ”€â”€ schemas/
                                                      â””â”€â”€ snowflake_credential.json (new)

setup/data_*.zip              â†’  (extracted)      â†’  data/source/{api,db}/
setup/gt.zip                  â†’  (extracted)      â†’  data/ground_truth/
```
## Workflow Overview

The complete ELT-Bench workflow consists of three phases:

### Phase 1: Setup (One-time)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. User fills Snowflake credentials                         â”‚
â”‚    â””â”€â”€ setup/destination/snowflake_credential.json         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2. Run setup/elt_setup.sh                                   â”‚
â”‚    â”œâ”€â”€ Downloads ZIPs (data_api, data_db, gt)              â”‚
â”‚    â”œâ”€â”€ Extracts to data/ directory                         â”‚
â”‚    â””â”€â”€ Runs write_config.py                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 3. write_config.py generates data/inputs/                   â”‚
â”‚    â”œâ”€â”€ Copies elt-bench/ â†’ data/inputs/                    â”‚
â”‚    â””â”€â”€ Creates snowflake_credential.json                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 4. Upload source data to Snowflake                          â”‚
â”‚    â””â”€â”€ python dev/snowflake-connector/upload_tables.py     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
          data/inputs/ ready for agents
          AIRBYTE_SCHEMA populated with source tables
          data/ground_truth/ populated with expected outputs
```

### Phase 2: Agent Execution (Iterative)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Agent operates in: data/inputs/<problem>/                   â”‚
â”‚                                                             â”‚
â”‚ 1. Reads problem requirements                               â”‚
â”‚    â”œâ”€â”€ config.yaml (source table info)                     â”‚
â”‚    â”œâ”€â”€ data_model.yaml (target schema)                     â”‚
â”‚    â”œâ”€â”€ schemas/ (source schemas)                           â”‚
â”‚    â””â”€â”€ snowflake_credential.json (connection)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2. Generates transformation code                            â”‚
â”‚    â””â”€â”€ Transform: dbt, SQL queries                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 3. Executes transformations                                 â”‚
â”‚    â”œâ”€â”€ Reads from AIRBYTE_SCHEMA (source tables)           â”‚
â”‚    â””â”€â”€ Writes to AIRBYTE_SCHEMA (target tables)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
            Transformed data in AIRBYTE_SCHEMA
```

### Phase 3: Evaluation (Validation)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Run: python evaluation/eva.py --folder <name> --example_index 0-99 â”‚
â”‚                                                             â”‚
â”‚ 1. Stage 1: Source Table Validation                        â”‚
â”‚    â”œâ”€â”€ Connects to Snowflake                               â”‚
â”‚    â”œâ”€â”€ Queries source tables in AIRBYTE_SCHEMA             â”‚
â”‚    â”œâ”€â”€ Compares vs data/ground_truth/<problem>/*.csv       â”‚
â”‚    â””â”€â”€ Logs to data/results/<folder>/eval_<problem>/stage1.log â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2. Stage 2: Transformation Validation                       â”‚
â”‚    â”œâ”€â”€ Runs SQL queries from evaluation/<problem>/*.sql    â”‚
â”‚    â”œâ”€â”€ Compares results vs ground truth                    â”‚
â”‚    â””â”€â”€ Logs to data/results/<folder>/eval_<problem>/stage2.log â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
        Results saved to data/results/<folder>/
```

### Directory Interaction Map
```
elt-bench/          (read-only templates)
    â†“ copied by write_config.py
data/inputs/        (agent workspace)
    â†“ agent reads
  Agent             (generates transformation code, executes SQL/DBT)
    â†“ writes to
Snowflake           (AIRBYTE_SCHEMA - reads source, writes target)
    â†“ queried by
evaluation/         (validation scripts + SQL queries)
    â†“ compares against
data/ground_truth/  (expected outputs)
    â†“ results written to
data/results/       (evaluation logs & scores)
```

## Environment Setup

### Prerequisites

Before starting, ensure you have installed:

| Tool | Purpose | Installation Guide |
|------|---------|-------------------|
| **Docker** | Runs agent containers | [Install Docker](https://docs.docker.com/get-docker/) |
| **Conda** | Python environment management | [Install Conda](https://docs.conda.io/projects/conda/en/stable/user-guide/install/index.html) |

### Setup Steps

#### 1. Install Conda Environment

```bash
# Create and activate conda environment
conda create -y -n elt
conda activate elt
conda install -y python=3.11

# Install Python dependencies
cd setup
pip install -r requirements.txt
```

#### 2. Configure Snowflake (Data Destination)

**2.1. Create Snowflake Resources**

Execute the SQL script in your Snowflake account:

```bash
# Copy contents of setup/destination/setup.sql
# Paste into Snowflake worksheet and run "Run All"
```

This creates:
- Database: `AIRBYTE_DATABASE`
- Warehouse: `AIRBYTE_WAREHOUSE`
- Role: `AIRBYTE_ROLE`
- User: `AIRBYTE_USER` (password: `Snowflake@123`)
- Schema: `AIRBYTE_SCHEMA`

**2.2. Fill Snowflake Credentials**

Edit `setup/destination/snowflake_credential.json`:

```json
{
  "account": "your-account.region.snowflakecomputing.com",
  "user": "AIRBYTE_USER",
  "password": "Snowflake@123"
}
```

**Important:** Use role `AIRBYTE_ROLE`, **not** `SYSADMIN`.

#### 3. Run Main Setup Script

Execute the setup script to:
- Download source data and ground truth (via `gdown`)
- Extract to `data/` directory
- Generate `data/inputs/` with credentials

```bash
cd setup
bash elt_setup.sh
```

**What this does:**
1. Downloads three ZIP files:
   - `data_api.zip` - API source data
   - `data_db.zip` - Database source data
   - `gt.zip` - Ground truth validation data
2. Extracts archives to `data/source/` and `data/ground_truth/`
3. Runs `write_config.py` to generate `data/inputs/` with Snowflake credentials

**Expected output structure:**
```
data/
â”œâ”€â”€ inputs/           # 100 problem folders with credentials
â”œâ”€â”€ source/
â”‚   â”œâ”€â”€ api/         # API data files
â”‚   â””â”€â”€ db/          # Database dumps
â””â”€â”€ ground_truth/    # Expected outputs (100 folders)
```

#### 4. Load Source Data to Snowflake

Upload source data to Snowflake's `AIRBYTE_SCHEMA`:

```bash
# From project root
python3 dev/snowflake-connector/upload_tables.py --example_index 0-99
```

**Parameters:**
- `--example_index 0-99`: Uploads all 100 problems (inclusive range)
- `--example_index 0-4`: Uploads only problems 0-4
- `--example_index 2,5,7`: Uploads specific problems

**Verification:**
After upload, check Snowflake for source tables in `AIRBYTE_DATABASE.AIRBYTE_SCHEMA`.

### Troubleshooting Setup

| Issue | Solution |
|-------|----------|
| `gdown` fails to download | Check internet connection, Google Drive quota |
| Snowflake connection fails | Verify credentials in `setup/destination/snowflake_credential.json` |
| `data/inputs/` empty after setup | Run `python3 setup/write_config.py` manually |

## Running Agents

See the `agents/` directory for instructions and examples:
- **`agents/spider-agent/`**: Spider-based agent implementation



## Evaluation

The evaluation framework validates agent performance across two stages:
- **Stage 1**: Source table validation - verifies source data is present in Snowflake's AIRBYTE_SCHEMA
- **Stage 2**: Transformation validation - validates transformed output against expected results

### Running Evaluation

```bash
cd evaluation
python eva.py --folder <folder_name> --example_index <range>
```

**Parameters:**

| Parameter | Description | Examples |
|-----------|-------------|----------|
| `--folder` | Name for this evaluation run | `spider_run_1`, `my_agent_test` |
| `--example_index` | Problems to evaluate | `0-99` (all), `0-4` (range), `2,5,7` (specific) |

**Examples:**

```bash
# Evaluate first 5 problems for testing
python eva.py --folder quick_test --example_index 0-4
```

```bash
# Evaluate specific problems
python eva.py --folder targeted_test --example_index 10,25,50,75
```

### Evaluation Output

Results are saved to `data/results/<folder>/`:

```
data/results/
â””â”€â”€ <folder_name>/              # Your evaluation run
    â”œâ”€â”€ eval_address/           # Results for "address" problem
    â”‚   â”œâ”€â”€ stage1.log          # EL validation logs
    â”‚   â””â”€â”€ stage2.log          # Transform validation logs
    â”œâ”€â”€ eval_airline/           # Results for "airline" problem
    â”‚   â”œâ”€â”€ stage1.log
    â”‚   â””â”€â”€ stage2.log
    â””â”€â”€ ... (for each evaluated problem)
```

### Automated Timestamping

**Note:** Evaluation runs are automatically timestamped to prevent overwriting previous results. You can specify a custom folder name with `--folder` for organization.
