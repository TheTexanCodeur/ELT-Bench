agent:
  templates:
    system_template: |-
      You are a data engineer skilled in databases, SQL, and building ELT pipelines.
      You are starting in the /workspace directory, which contains all the necessary information for your tasks. However, you are only allowed to modify files in /workspace/elt.
      Your goal is to build an ELT pipeline by extracting data from multiple sources, such as custom APIs, PostgreSQL, MongoDB, flat files, and the cloud service S3.
      The extracted data will be loaded into a target system, Snowflake, followed by writing transformation queries to construct final tables for downstream use.
      This task is divided into two stages:
      1. Data Extraction and Loading – Using Airbyte's Terraform provider.
      2. Data Transformation – Using the DBT Project workflow for Snowflake.

      RESPONSE FORMAT:
      Your shell prompt is formatted as follows:
      (Open file: <path>)
      (Current directory: <cwd>)
      bash-$

      First, you should _always_ include a general thought about what you're going to do next.
      Then, for every response, you must include exactly _ONE_ tool call/function call.

      Remember, you should always include a _SINGLE_ tool call/function call and then wait for a response from the shell before continuing with more discussion and commands. Everything you include in the DISCUSSION section will be saved for future reference.

    instance_template: |-
      We're currently building the transformation phase of an ELT pipeline using DBT. All data has already been extracted and loaded into Snowflake.

      INSTRUCTIONS:
      # Data Transformation Hints#
      1. Initialize the DBT Project: Set up a new DBT project by configuring it with /workspace/config.yaml, and remove the example directory under the models directory.
      • Important: Configure DBT to write all transformed tables to the AIRBYTE_SCHEMA schema. The source raw tables are in AIRBYTE_SCHEMA, and all output tables MUST also be written to AIRBYTE_SCHEMA. Do not create or use any other schema (such as ANALYTICS, DBT_SCHEMA, etc.).
      2. Understand the Data Model: Review data_model.yaml in /workspace to understand the required data models and their column descriptions. Then, write SQL queries to generate these defined data models, referring to the files in the /workspace/schemas directory to understand the schemas of source tables.
      • Important: Write a separate query for each data model, and if using any DBT project variables, ensure they have already been declared.
      3. Validate Table Locations: Ensure all SQL queries reference the correct database and schema names for source tables. All source tables are located in AIRBYTE_SCHEMA. If you encounter a "table not found" error, refer to /workspace/config.yaml to obtain the correct configuration or use SF_GET_TABLES to check all available tables in the database.
      4. Run the DBT Project: Execute `dbt run` to apply transformations and generate the final data models in Snowflake in the AIRBYTE_SCHEMA schema, fixing any errors reported by DBT.
      5. Terminate the Task: Terminate the task if all transformations align with data_model.yaml and the final tables in Snowflake are accurate, verified, and located in AIRBYTE_SCHEMA. Alternatively, terminate if you are unable to resolve the issues after multiple retries.

      (Open file: {{open_file}})
      (Current directory: {{working_dir}})
      bash-$
    next_step_template: |-
      {{observation}}
      (Open file: {{open_file}})
      (Current directory: {{working_dir}})
      bash-$
    next_step_no_output_template: |-
      Your command ran successfully and did not produce any output.
      (Open file: {{open_file}})
      (Current directory: {{working_dir}})
      bash-$
    put_demos_in_history: true
  tools:
    env_variables:
      WINDOW: 100
      OVERLAP: 2
    bundles:
      - path: tools/registry
      - path: tools/defaults
      - path: tools/search
      # - path: tools/edit_linting
      - path: tools/edit_replace
      - path: tools/submit
    enable_bash_tool: true
    parse_function:
      type: function_calling
  history_processors:
    - type: last_n_observations
      n: 25