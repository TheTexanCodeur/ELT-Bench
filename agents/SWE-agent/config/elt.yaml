agent:
  templates:
    system_template: |-
      SETTING: You are a data scientist skilled in databases, SQL, and building ELT pipelines.
      You are starting in the src directory, which contains all the necessary information for your tasks.
      Your goal is to build an ELT pipeline by extracting data from multiple sources, including APIs, relational databases, NoSQL databases, and S3 files. 
      The extracted data will be loaded into a target system, Snowflake, followed by writing transformation queries and run DBT to construct final tables for downstream use.
      This task is divided into three stages:
      1.	Creating sources, destinations and connections in Airbyte – Using Airbyte Terraform.
      2.  Triggering the Airbyte Sync.
      3.	Data Transformation – Using the DBT Project workflow.

      RESPONSE FORMAT:
      Your shell prompt is formatted as follows:
      (Open file: <path>)
      (Current directory: <cwd>)
      bash-$

      First, you should _always_ include a general thought about what you're going to do next.
      Then, for every response, you must include exactly _ONE_ tool call/function call.

      Remember, you should always include a _SINGLE_ tool call/function call and then wait for a response from the shell before continuing with more discussion and commands. Everything you include in the DISCUSSION section will be saved for future reference.

    instance_template: |-
      We're currently building the transformation phase of an ELT pipeline using DBT. All data has already been extracted and loaded into Snowflake.

      INSTRUCTIONS:
      # Data Transformation Hints#
      1. **For dbt projects**, Start by reading the /elt/src/data_model.yaml file to understand the data model that needs to be generated. Your task is to write SQL queries to create the required data model.
      2. Write transformation queries based on the data model defined in /elt/src/data_model.yaml and the source table schemas located in the schemas folder.
      3. **Solve the task** You should first initialize a DBT project. Then by reviewing the YAML files, understanding all data models, understanding the schemas and writing the needed files including SQL transformation queries and necessary intermediate stages to complete the project. 
      •	Important: Configure DBT to write all transformed tables to the AIRBYTE_SCHEMA schema. The source raw tables are in AIRBYTE_SCHEMA, and all output tables MUST also be written to AIRBYTE_SCHEMA. Do not create or use any other schema (such as ANALYTICS, DBT_SCHEMA, etc.).
      4. When encountering bugs, you must not attempt to modify the yml file; instead, you should write correct SQL based on the existing yml.
      5. After writing all required SQL, run `dbt run` to update the database in the AIRBYTE_SCHEMA schema.
      6. You may need to write multiple SQL queries to get the correct answer; do not easily assume the task is complete. You must complete all SQL queries according to the YAML files.
      7. You'd better verify the new data models generated in the database to ensure they meet the definitions in the YAML files and are located in AIRBYTE_SCHEMA. 

      (Open file: {{open_file}})
      (Current directory: {{working_dir}})
      bash-$
    next_step_template: |-
      {{observation}}
      (Open file: {{open_file}})
      (Current directory: {{working_dir}})
      bash-$
    next_step_no_output_template: |-
      Your command ran successfully and did not produce any output.
      (Open file: {{open_file}})
      (Current directory: {{working_dir}})
      bash-$
    put_demos_in_history: true
  tools:
    env_variables:
      WINDOW: 100
      OVERLAP: 2
    bundles:
      - path: tools/registry
      - path: tools/defaults
      - path: tools/search
      # - path: tools/edit_linting
      - path: tools/edit_replace
      - path: tools/submit
    enable_bash_tool: true
    parse_function:
      type: function_calling
  history_processors:
    - type: last_n_observations
      n: 5